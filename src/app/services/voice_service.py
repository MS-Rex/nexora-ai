import os
import io
import tempfile
import asyncio
import logging
import re
from typing import Optional, Tuple
import numpy as np
import whisper
from gtts import gTTS
from pydub import AudioSegment
from pydub.silence import split_on_silence
import soundfile as sf
import librosa
from src.app.agents.orchestrator_agent import orchestrator_agent

logger = logging.getLogger(__name__)


class VoiceService:
    """
    Voice service for handling speech-to-text, AI response generation, and text-to-speech
    """
    
    def __init__(self):
        self.whisper_model = None
        self.orchestrator_agent = orchestrator_agent
        self._load_models()
    
    def _load_models(self):
        """Load Whisper model and initialize orchestrator agent"""
        try:
            # Load Whisper model (using base model for balance of speed/accuracy)
            logger.info("Loading Whisper model...")
            self.whisper_model = whisper.load_model("base")
            logger.info("‚úÖ Whisper model loaded successfully")
            
            # Check if orchestrator agent is available
            if self.orchestrator_agent:
                logger.info("‚úÖ Orchestrator agent initialized successfully")
            else:
                logger.warning("‚ùå Orchestrator agent not available")
                
        except Exception as e:
            logger.error(f"‚ùå Error loading models: {e}")
    
    async def transcribe_audio(self, audio_data: bytes) -> str:
        """
        Transcribe audio bytes to text using Whisper
        
        Args:
            audio_data: Raw audio bytes
            
        Returns:
            Transcribed text
        """
        try:
            logger.info(f"üì• Received audio data: {len(audio_data)} bytes")
            
            # Create temporary file for audio processing - use generic extension first
            with tempfile.NamedTemporaryFile(suffix=".webm", delete=False) as temp_file:
                temp_file.write(audio_data)
                temp_filename = temp_file.name
            
            logger.info(f"üíæ Saved audio to temp file: {temp_filename}")
            
            try:
                # Try to load the audio file with pydub
                try:
                    audio_segment = AudioSegment.from_file(temp_filename)
                    logger.info(f"üéµ Audio loaded: {len(audio_segment)}ms, {audio_segment.channels} channels, {audio_segment.frame_rate}Hz")
                except Exception as e:
                    logger.error(f"‚ùå Error loading audio with pydub: {e}")
                    # Try loading as WebM specifically
                    audio_segment = AudioSegment.from_file(temp_filename, format="webm")
                    logger.info(f"üéµ Audio loaded as WebM: {len(audio_segment)}ms")
                
                # Ensure audio is at least 1 second long
                if len(audio_segment) < 1000:  # Less than 1 second
                    logger.warning(f"‚ö†Ô∏è  Audio too short: {len(audio_segment)}ms")
                    return ""
                
                # Convert to mono and set appropriate sample rate
                audio_segment = audio_segment.set_channels(1)
                audio_segment = audio_segment.set_frame_rate(16000)
                
                # Remove silence and normalize
                audio_segment = audio_segment.strip_silence(silence_thresh=-40)
                audio_segment = audio_segment.normalize()
                
                # Export to WAV file for Whisper (Whisper works best with WAV)
                processed_filename = temp_filename.replace(".webm", "_processed.wav")
                audio_segment.export(processed_filename, format="wav", parameters=["-ar", "16000", "-ac", "1"])
                
                logger.info(f"üîÑ Processed audio saved to: {processed_filename}")
                
                # Transcribe using Whisper
                logger.info("üéØ Starting Whisper transcription...")
                loop = asyncio.get_event_loop()
                result = await loop.run_in_executor(
                    None, 
                    self._transcribe_with_whisper, 
                    processed_filename
                )
                
                transcribed_text = result["text"].strip()
                logger.info(f"üéôÔ∏è Transcribed: '{transcribed_text}'")
                
                # Check if transcription is meaningful
                if len(transcribed_text) < 2:
                    logger.warning("‚ö†Ô∏è  Transcription too short, likely silence or noise")
                    return ""
                
                return transcribed_text
                
            finally:
                # Clean up temporary files
                try:
                    os.unlink(temp_filename)
                    if 'processed_filename' in locals():
                        os.unlink(processed_filename)
                except Exception as cleanup_error:
                    logger.warning(f"‚ö†Ô∏è  Error cleaning up temp files: {cleanup_error}")
                    
        except Exception as e:
            logger.error(f"‚ùå Error transcribing audio: {e}")
            import traceback
            logger.error(f"Full traceback: {traceback.format_exc()}")
            return ""
    
    def _transcribe_with_whisper(self, audio_file_path: str) -> dict:
        """Helper method to run Whisper transcription synchronously"""
        try:
            result = self.whisper_model.transcribe(
                audio_file_path,
                language="en",  # Force English for better accuracy
                task="transcribe",
                verbose=False
            )
            return result
        except Exception as e:
            logger.error(f"‚ùå Whisper transcription failed: {e}")
            return {"text": ""}
    
    async def generate_response(self, text: str, user_id: Optional[str] = None) -> str:
        """
        Generate AI response using OrchestratorAgent
        
        Args:
            text: Input text from user
            user_id: Optional user ID for context
            
        Returns:
            AI generated response
        """
        try:
            if not self.orchestrator_agent:
                return "Sorry, I'm not properly configured to generate responses."
            
            logger.info(f"ü§ñ Sending to OrchestratorAgent: '{text}'")
            
            # Use the orchestrator agent to handle the query
            response = await self.orchestrator_agent.handle_query(
                message=text,
                user_id=user_id
            )
            
            logger.info(f"ü§ñ OrchestratorAgent Response: {response}")
            
            return response
            
        except Exception as e:
            logger.error(f"‚ùå Error generating response with OrchestratorAgent: {e}")
            import traceback
            logger.error(f"Full traceback: {traceback.format_exc()}")
            return "Sorry, I encountered an error while generating a response."
    
    async def text_to_speech(self, text: str, language: str = "en") -> bytes:
        """
        Convert text to speech using gTTS
        
        Args:
            text: Text to convert to speech
            language: Language code (default: 'en')
            
        Returns:
            Audio bytes in MP3 format
        """
        try:
            # Detect if text likely contains markdown and clean it if needed
            if any(marker in text for marker in ['#', '```', '`', '*', '_', '[', ']', '->']):
                logger.info("üìù Detected markdown in text for speech, cleaning...")
                text = self._clean_markdown_for_speech(text)
            
            # Create gTTS object
            tts = gTTS(text=text, lang=language, slow=False)
            
            # Save to bytes buffer
            audio_buffer = io.BytesIO()
            tts.write_to_fp(audio_buffer)
            audio_buffer.seek(0)
            
            audio_bytes = audio_buffer.getvalue()
            logger.info(f"üîä Generated speech for: {text[:50]}...")
            
            return audio_bytes
            
        except Exception as e:
            logger.error(f"‚ùå Error generating speech: {e}")
            return b""
    
    def preprocess_audio_chunk(self, audio_chunk: bytes) -> bytes:
        """
        Preprocess audio chunk for better transcription
        
        Args:
            audio_chunk: Raw audio bytes
            
        Returns:
            Processed audio bytes
        """
        try:
            logger.info(f"üîÑ Preprocessing audio chunk: {len(audio_chunk)} bytes")
            
            # Try to load audio with different formats
            audio_segment = None
            for fmt in ['webm', 'wav', 'mp3', 'm4a']:
                try:
                    audio_segment = AudioSegment.from_file(io.BytesIO(audio_chunk), format=fmt)
                    logger.info(f"‚úÖ Successfully loaded audio as {fmt}")
                    break
                except Exception as e:
                    logger.debug(f"Failed to load as {fmt}: {e}")
                    continue
            
            if audio_segment is None:
                logger.error("‚ùå Could not load audio in any supported format")
                return audio_chunk
            
            # Basic audio validation
            if len(audio_segment) < 500:  # Less than 0.5 seconds
                logger.warning(f"‚ö†Ô∏è  Audio too short for preprocessing: {len(audio_segment)}ms")
                return audio_chunk
            
            # Remove silence at the beginning and end (more aggressive)
            audio_segment = audio_segment.strip_silence(silence_thresh=-35, silence_chunk_len=300)
            
            # Normalize audio levels
            audio_segment = audio_segment.normalize()
            
            # Apply some basic noise reduction by removing very quiet parts
            # This helps with browser audio capture issues
            if audio_segment.max_dBFS < -30:  # Very quiet audio
                logger.warning("‚ö†Ô∏è  Audio appears very quiet, boosting volume")
                audio_segment = audio_segment + (20 - audio_segment.max_dBFS)
            
            # Convert back to bytes in WAV format for better compatibility
            output_buffer = io.BytesIO()
            audio_segment.export(output_buffer, format="wav", parameters=["-ar", "16000", "-ac", "1"])
            output_buffer.seek(0)
            
            processed_bytes = output_buffer.getvalue()
            logger.info(f"‚úÖ Audio preprocessed: {len(processed_bytes)} bytes")
            
            return processed_bytes
            
        except Exception as e:
            logger.error(f"‚ùå Error preprocessing audio: {e}")
            import traceback
            logger.error(f"Full traceback: {traceback.format_exc()}")
            return audio_chunk
    
    def _clean_markdown_for_speech(self, text: str) -> str:
        """
        Clean markdown formatting for better speech synthesis
        
        Args:
            text: Text that may contain markdown formatting
            
        Returns:
            Cleaned text suitable for speech synthesis
        """
        # Remove code blocks (```...```)
        text = re.sub(r'```[\s\S]*?```', '', text)
        
        # Remove inline code blocks (`...`)
        text = re.sub(r'`([^`]*)`', r'\1', text)
        
        # Remove headers (# Header)
        text = re.sub(r'^\s*#{1,6}\s+(.*?)$', r'\1', text, flags=re.MULTILINE)
        
        # Convert links [text](url) to just text
        text = re.sub(r'\[([^\]]+)\]\([^)]+\)', r'\1', text)
        
        # Remove bold and italic markers
        text = re.sub(r'\*\*(.*?)\*\*', r'\1', text)
        text = re.sub(r'__(.*?)__', r'\1', text)
        text = re.sub(r'\*(.*?)\*', r'\1', text)
        text = re.sub(r'_(.*?)_', r'\1', text)
        
        # Remove bullet points and numbered lists
        text = re.sub(r'^\s*[-*+]\s+', '', text, flags=re.MULTILINE)
        text = re.sub(r'^\s*\d+\.\s+', '', text, flags=re.MULTILINE)
        
        # Remove horizontal rules
        text = re.sub(r'^\s*[-*_]{3,}\s*$', '', text, flags=re.MULTILINE)
        
        # Remove HTML tags
        text = re.sub(r'<[^>]*>', '', text)
        
        # Remove blockquote markers
        text = re.sub(r'^\s*>\s+', '', text, flags=re.MULTILINE)
        
        # Collapse multiple blank lines
        text = re.sub(r'\n{3,}', '\n\n', text)
        
        return text.strip()

    async def process_voice_message(self, audio_data: bytes, user_id: Optional[str] = None) -> Tuple[str, bytes]:
        """
        Complete voice-to-voice processing pipeline
        
        Args:
            audio_data: Raw audio bytes from user
            user_id: Optional user ID for context
            
        Returns:
            Tuple of (transcribed_text, response_audio_bytes)
        """
        try:
            logger.info(f"üöÄ Starting voice processing pipeline with {len(audio_data)} bytes for user: {user_id}")
            
            # Step 1: Preprocess audio
            logger.info("üìù Step 1: Preprocessing audio...")
            processed_audio = self.preprocess_audio_chunk(audio_data)
            
            # Step 2: Transcribe speech to text
            logger.info("üìù Step 2: Transcribing audio to text...")
            transcribed_text = await self.transcribe_audio(processed_audio)
            
            if not transcribed_text:
                logger.warning("‚ö†Ô∏è  No text transcribed from audio")
                error_msg = "Sorry, I couldn't understand your message. Please try speaking more clearly and try again."
                error_audio = await self.text_to_speech(error_msg)
                return "", error_audio
            
            logger.info(f"‚úÖ Successfully transcribed: '{transcribed_text}'")
            
            # Step 3: Generate AI response using OrchestratorAgent
            logger.info("üìù Step 3: Generating AI response via OrchestratorAgent...")
            response_text = await self.generate_response(transcribed_text, user_id)
            
            # Clean markdown from response text for voice output only
            cleaned_response_text = self._clean_markdown_for_speech(response_text)
            logger.info(f"‚úÖ Cleaned markdown for speech: original length {len(response_text)}, cleaned length {len(cleaned_response_text)}")
            
            # Step 4: Convert response to speech
            logger.info("üìù Step 4: Converting response to speech...")
            response_audio = await self.text_to_speech(cleaned_response_text)
            
            logger.info("üéâ Voice processing pipeline completed successfully!")
            return transcribed_text, response_audio
            
        except Exception as e:
            logger.error(f"‚ùå Error in voice processing pipeline: {e}")
            import traceback
            logger.error(f"Full traceback: {traceback.format_exc()}")
            error_msg = "Sorry, I encountered an error processing your message."
            error_audio = await self.text_to_speech(error_msg)
            return "", error_audio


# Global voice service instance
voice_service = VoiceService() 